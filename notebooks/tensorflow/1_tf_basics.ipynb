{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![prova](https://upload.wikimedia.org/wikipedia/commons/thumb/a/ab/TensorFlow_logo.svg/1200px-TensorFlow_logo.svg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Manipulation Basics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Creation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 0-dimensional tensor is a __scalar__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar = tf.constant(0)\n",
    "print(f\"Value of the tensor = {scalar}\")\n",
    "print(f\"Number of dimensions = {len(scalar.shape)}\")\n",
    "print(f\"Tensor's shape = {scalar.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 1-dimensional tensor is a __vector__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = tf.constant([1, 2, 3])\n",
    "print(f\"Value of the tensor = {vector}\")\n",
    "print(f\"Number of dimensions = {len(vector.shape)}\")\n",
    "print(f\"Tensor's shape = {vector.shape}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 2-dimensional tensor is a __matrix__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = tf.constant(\n",
    "    [[1, 2, 3],\n",
    "     [4, 5, 6],\n",
    "     [7, 8, 9]]\n",
    ")\n",
    "print(f\"Value of the tensor = \\n {matrix}\")\n",
    "print(f\"Number of dimensions = {len(matrix.shape)}\")\n",
    "print(f\"Tensor's shape = {matrix.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generalize tensors to __n dimensions__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3\n",
    "tensor = tf.random.normal(tuple(3 for _ in range(n)))\n",
    "print(f\"Value of the tensor = \\n {tensor}\")\n",
    "print(f\"Number of dimensions = {len(tensor.shape)}\")\n",
    "print(f\"Tensor's shape = {tensor.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each tensor is characterized also by a __data type__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor.dtype"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which can be cast to others (with the clear consequences on the numerical representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_tensor = tf.cast(tensor, dtype=tf.int32)\n",
    "print(int_tensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor indexing e slicing\n",
    "\n",
    "Tensors can be __indexed__ (i.e., ``tensor[i, :, :]``) or __sliced__ (i.e., ``tensor[:i, ...]``)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing a tensor reduces its dimensionality depending to the number of \"free\" dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Scalar = {tensor[0, 1, 0]}\")\n",
    "print(f\"Vector = {tensor[:, 0, -1]}\") # : means all elements in that dimension\n",
    "print(f\"Matrix = {tensor[:, 2]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slicing reduces the size of the sliced dimension. With this approach, only contiguous slices can be taken. To get scattered slices, use [``tf.gather``](https://www.tensorflow.org/api_docs/python/tf/gather)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_slice = tensor[1:]\n",
    "print(f\"tensor_slice = {tensor_slice}\")\n",
    "print(f\"Shape of tensor_slice = {tensor_slice.shape}\")\n",
    "print(f\"Number of dimensions = {len(tensor_slice.shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_slice = tensor[1:, :, -1:]\n",
    "print(f\"tensor_slice = {tensor_slice}\")\n",
    "print(f\"Shape of tensor_slice = {tensor_slice.shape}\")\n",
    "print(f\"Number of dimensions = {len(tensor_slice.shape)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing and slicing can be also mixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_slice = tensor[1:, :, 2]\n",
    "print(f\"tensor_slice = {tensor_slice}\")\n",
    "print(f\"Shape of tensor_slice = {tensor_slice.shape}\")\n",
    "print(f\"Number of dimensions = {len(tensor_slice.shape)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concat and stack\n",
    "\n",
    "- ``tf.concat``: concatenates n tensors on the `axis` dimension. All the dimensions of the input tensors, except for the `axis` dimension, must match.\n",
    "- ``tf.stack``: stacks n tensors on the `axis` dimension, which is added for the resulting tensor. All the dimensions of the input tensors must match. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_1 = tf.random.normal((2, 3, 4))\n",
    "tensor_2 = tf.random.uniform((2, 6, 4))\n",
    "concat_tensor = tf.concat([tensor_1, tensor_2], axis=1)\n",
    "print(f\"Shape = {concat_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_2 = tf.random.uniform((2, 3, 4))\n",
    "stacked_tensor = tf.stack([tensor_1, tensor_2], axis=1)\n",
    "print(f\"Shape = {stacked_tensor.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn!\n",
    "\n",
    "1. Create a tensor with random numbers of shape (2, 5, 3);\n",
    "2. Get the last element on dimension 1;\n",
    "3. Put it at the beginning of dimension 1.\n",
    "\n",
    "_Note_: use the placeholders on the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_tensor = None\n",
    "final_tensor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if final_tensor.shape == (2, 5, 3):\n",
    "    if tf.reduce_all(final_tensor[:, 0] == init_tensor[:, -1]):\n",
    "        print(\"Good job!\")\n",
    "    else:\n",
    "        print(\"Mh, correct dimensions but wrong values\")\n",
    "else:\n",
    "    print(f\"Wrong, dimensions are {init_tensor.shape} and {final_tensor.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor operations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``+, *, /, -`` are overloaded to support tensor operations. All the operations are element-wise, support broadcasting for the non-matching dimensions (i.e., (2, 1, 2) + (2, 3, 2) --> (2, 3, 2))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = tf.random.normal((3, 3, 3))\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_1 = tf.random.normal((3, 2))\n",
    "tensor_2 = tf.random.normal((3, 2))\n",
    "tensor_1 + tensor_2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`@` defines the dot product between two tensors. The inner dimensions must match the criterion for the dot product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_1 @ tf.transpose(tensor_2, [1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_1 = tf.random.normal((10, 3, 2))\n",
    "tensor_2 = tf.random.normal((10, 3, 2))\n",
    "tensor_1 @ tf.transpose(tensor_2, [0, 2, 1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning with TensorFlow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digit Classification with a Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_x, train_y), (test_x, test_y) = tf.keras.datasets.mnist.load_data(path='ds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Label is {train_y[0]}\")\n",
    "Image.fromarray(train_x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images need to be flattened to be taken as input by a linear classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = tf.reshape(train_x, [train_x.shape[0], -1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to __rescale__ the values to the interval [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x / 255"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the linear model requires only a weight matrix and a bias vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, b = tf.Variable(tf.random.normal((784, 10))), tf.Variable(tf.zeros(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = tf.nn.softmax(train_x @ W + b, axis=-1)\n",
    "prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our linear model, let's define the tools for the optimization, i.e., __the optimizer and the loss function__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-1)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A training loop with TensorFlow is shaped as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "epochs = 500 \n",
    "\n",
    "for e in range(epochs):\n",
    "    # The GradientTape context records every operation applied to tensors\n",
    "    # inside the context. The tape can then be used to compute the gradient\n",
    "    # of a computation with respect to the tensors it has \"watched\".\n",
    "    with tf.GradientTape() as tape:\n",
    "        prediction = tf.nn.softmax(train_x @ W + b, axis=-1)\n",
    "        loss_value = loss_fn(train_y, prediction)\n",
    "    \n",
    "    # We compute the gradient of the loss with respect to the parameters\n",
    "    # of the model\n",
    "    grads = tape.gradient(loss_value, [W, b])\n",
    "\n",
    "    # We apply the gradient to the parameters of the model\n",
    "    optimizer.apply_gradients(zip(grads, [W, b]))\n",
    "\n",
    "    # We print the loss every 20 epochs\n",
    "    prediction = tf.nn.softmax(train_x @ W + b, axis=-1)\n",
    "    if e % 20 == 0:\n",
    "        print(f\"Epoch {e}: accuracy = {accuracy_score(train_y, tf.argmax(prediction, axis=-1))}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, if we use Adam as optimization approach, the performance in the learning phase increases dramatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, b = tf.Variable(tf.random.normal((784, 10))), tf.Variable(tf.zeros(10))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "epochs = 500\n",
    "\n",
    "for e in range(epochs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        prediction = tf.nn.softmax(train_x @ W + b, axis=-1)\n",
    "        loss_value = loss_fn(train_y, prediction)\n",
    "    \n",
    "    grads = tape.gradient(loss_value, [W, b])\n",
    "    optimizer.apply_gradients(zip(grads, [W, b]))\n",
    "\n",
    "    if e % 20 == 0:\n",
    "        prediction = tf.nn.softmax(train_x @ W + b, axis=-1)\n",
    "        print(f\"Epoca {e}: accuratezza = {accuracy_score(train_y, tf.argmax(prediction, axis=-1))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your turn!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to implement your own training loop with a linear regression on the Boston Housing dataset. \n",
    "\n",
    "Notes:\n",
    "1. It is a regression problem, so you need to use a different loss function;\n",
    "2. MinMax Scaling may not be the most appropriate way to rescale the input features (maybe sklearn's StandardScaler?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_x, train_y), _ = tf.keras.datasets.boston_housing.load_data(\"./ds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tf_mela')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3917c018cfd92834e25e8ac39593c8ba50cba5d103d91e5ca7d5a867c45ae1af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
